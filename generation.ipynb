{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bd3c6-7403-481b-8e9d-6995bdb6bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import logging\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, CLIPImageProcessor\n",
    "from llava_phi.model import LlavaPhiForCausalLM\n",
    "from llava_phi.constants import DEFAULT_IMAGE_TOKEN\n",
    "from llava_phi.conversation import conv_templates\n",
    "from llava_phi.utils import disable_torch_init\n",
    "from transformers.generation.utils import GenerationMixin\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "\n",
    "MODEL_PATH = \"/media/volume/Slava/Dual-View-Slava-Final\"\n",
    "IMAGE_FOLDER = \"/media/volume/Slava/MIMIC_Dataset224\"\n",
    "INPUT_JSON = \"slava_llava_split_test.json\"\n",
    "OUTPUT_JSON = \"Duaal_slava_llava_predict.json\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TORCH_DTYPE = torch.float16\n",
    "MANUAL_SEED = 42\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "REPORT_INSTRUCTIONS = [\n",
    "    \"<image> You are a board-certified radiologist. Analyze the frontal and lateral chest X-rays. Identify and describe all radiographic findings, including location, size, density, and distribution. Provide a structured report with FINDINGS and IMPRESSION. Ensure clinical relevance and diagnostic precision.\",\n",
    "    \"<image> Evaluate the dual-view CXRs thoroughly. Identify all signs of disease, subtle or prominent. For each finding, describe anatomical position, severity, and possible differential diagnoses. Conclude with a well-reasoned and structured IMPRESSION.\",\n",
    "    \"<image> Generate a detailed radiology report using standard clinical format. Include all abnormal and incidental findings in the FINDINGS section. Summarize key diagnostic insights in the IMPRESSION. Avoid generic language; favor precise anatomical and pathological descriptors.\",\n",
    "    \"<image> Carefully examine the frontal and lateral chest X-rays. Write a complete radiology report structured into: FINDINGS (organized by organ system and zones) and IMPRESSION (summary with clinical prioritization). Include all deviations from normal, no matter how subtle.\",\n",
    "    \"<image> Interpret these dual-view CXRs. Document all radiologic abnormalities by describing their appearance, location (lung zones, mediastinum, pleura, bones), and clinical implications. Format the response as a formal radiology report with clear headings for FINDINGS and IMPRESSION.\",\n",
    "    \"<image> Examine the provided chest X-rays from both frontal and lateral views. Accurately identify and describe any abnormal radiologic signs, even subtle or borderline cases. Structure the report with FINDINGS and a diagnostic IMPRESSION.\",\n",
    "    \"<image> Perform a radiological assessment of these dual-view CXRs. List all notable observations and pathological signs, with attention to anatomical detail and clinical context. Format the response as a full report: FINDINGS followed by IMPRESSION.\",\n",
    "    \"<image> Review the chest radiographs and generate a report that includes all visible abnormalities. Pay attention to symmetry, lung markings, cardiac silhouette, and bony structures. Use the standard format with FINDINGS and IMPRESSION.\",\n",
    "    \"<image> You are an expert thoracic radiologist. Describe all pathological and incidental findings in these frontal and lateral CXRs. Be thorough and concise. Conclude with a structured IMPRESSION summarizing the clinical picture.\",\n",
    "    \"<image> Analyze these chest X-rays using your clinical expertise. Report every abnormality using specific radiologic terminology. Clearly differentiate FINDINGS and IMPRESSION. Include zone-wise, side-wise, and severity-based descriptions.\"\n",
    "]\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LlavaWithGenerate(LlavaPhiForCausalLM, GenerationMixin):\n",
    "    def generate(self, *args, **kwargs):\n",
    "        self.image_features = None\n",
    "        self.images = kwargs.pop(\"images\", None)\n",
    "        return super().generate(*args, **kwargs)\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    disable_torch_init()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    if \"<image>\" not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = LlavaWithGenerate.from_pretrained(MODEL_PATH, torch_dtype=TORCH_DTYPE, low_cpu_mem_usage=True)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    model.model.bypass_vision_tower = False\n",
    "    image_token_id = tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "    assert image_token_id < model.get_input_embeddings().num_embeddings\n",
    "\n",
    "    if torch.__version__ >= \"2\" and torch.cuda.is_available():\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"torch.compile() failed: {e}\")\n",
    "\n",
    "    return model, tokenizer, image_token_id\n",
    "\n",
    "\n",
    "def prepare_conversation(prompt):\n",
    "    template_name = next((name for name in [\"llava_phi\", \"phi\", \"v0\", \"med\", \"vicuna_v1\"] if name in conv_templates), None)\n",
    "    conv = conv_templates[template_name].copy()\n",
    "    conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "\n",
    "IMAGE_PROCESSOR = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "\n",
    "def expand2square(pil_img, background_color=(0, 0, 0)):\n",
    "    w, h = pil_img.size\n",
    "    if w == h:\n",
    "        return pil_img\n",
    "    elif w > h:\n",
    "        result = Image.new(pil_img.mode, (w, w), background_color)\n",
    "        result.paste(pil_img, (0, (w - h) // 2))\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (h, h), background_color)\n",
    "        result.paste(pil_img, ((h - w) // 2, 0))\n",
    "    return result\n",
    "\n",
    "def prepare_images(frontal_path, lateral_path):\n",
    "    image_paths = [frontal_path, lateral_path]\n",
    "    images = []\n",
    "    bg_color = tuple(int(x * 255) for x in IMAGE_PROCESSOR.image_mean)\n",
    "    for img_path in image_paths:\n",
    "        with Image.open(os.path.join(IMAGE_FOLDER, img_path)) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "            img = expand2square(img, bg_color)\n",
    "            image_tensor = IMAGE_PROCESSOR(img, return_tensors=\"pt\")['pixel_values'][0]\n",
    "            images.append(image_tensor)\n",
    "    return torch.stack(images).contiguous()\n",
    "\n",
    "\n",
    "def tokenizer_image_token(prompt, tokenizer, image_token_index, return_tensors=None):\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors == 'pt':\n",
    "        return torch.tensor(input_ids, dtype=torch.long)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, images, prompt, reference, image_token_index):\n",
    "    full_prompt = prepare_conversation(prompt)\n",
    "    input_ids = tokenizer_image_token(full_prompt, tokenizer, image_token_index, return_tensors=\"pt\").unsqueeze(0).to(DEVICE)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long().to(DEVICE)\n",
    "    images = images.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode(), torch.autocast(device_type='cuda', dtype=TORCH_DTYPE):\n",
    "        try:\n",
    "            output_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                do_sample=True,\n",
    "                temperature=0.2,\n",
    "                top_p=0.8,\n",
    "                max_new_tokens=1024,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            if \"indexSelectLargeIndex\" in str(e):\n",
    "                return \"[ERROR: CUDA index bug]\"\n",
    "            raise e\n",
    "\n",
    "    gen_tokens = output_ids[0, input_ids.shape[1]:]\n",
    "    return tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def process_validation():\n",
    "    logger.info(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer, image_token_index = load_model_and_tokenizer()\n",
    "    torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "    logger.info(\"Reading annotation file...\")\n",
    "    with open(INPUT_JSON, \"r\") as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    logger.info(\"Checking for existing output...\")\n",
    "    existing_predictions = {}\n",
    "    if os.path.exists(OUTPUT_JSON):\n",
    "        with open(OUTPUT_JSON, \"r\") as f:\n",
    "            for item in json.load(f):\n",
    "                key = f\"{item['frontal']}|{item['lateral']}\"\n",
    "                existing_predictions[key] = item\n",
    "\n",
    "    processed_keys = set(existing_predictions.keys())\n",
    "    logger.info(f\"Found {len(processed_keys)} previously processed entries. Skipping them...\")\n",
    "\n",
    "    for entry in tqdm(val_data, desc=\"Processing\"):\n",
    "        try:\n",
    "            findings = entry.get(\"findings\")\n",
    "            impression = entry.get(\"impression\")\n",
    "            reference_text = f\"{findings}. IMPRESSION: {impression}\" if findings and impression else \"\"\n",
    "            frontal = entry.get(\"frontal\")\n",
    "            lateral = entry.get(\"lateral\")\n",
    "            key = f\"{frontal}|{lateral}\"\n",
    "\n",
    "            if key in processed_keys:\n",
    "                continue\n",
    "            if not frontal or not lateral or not reference_text:\n",
    "                continue\n",
    "\n",
    "            prompt = random.choice(REPORT_INSTRUCTIONS)\n",
    "            images = prepare_images(frontal, lateral)\n",
    "            prediction = generate_response(model, tokenizer, images, prompt, reference_text, image_token_index)\n",
    "\n",
    "            result = {\n",
    "                \"frontal\": frontal,\n",
    "                \"lateral\": lateral,\n",
    "                \"prompt\": prompt,\n",
    "                \"reference\": reference_text,\n",
    "                \"prediction\": prediction\n",
    "            }\n",
    "\n",
    "            existing_predictions[key] = result\n",
    "            with open(OUTPUT_JSON, \"w\") as f:\n",
    "                json.dump(list(existing_predictions.values()), f, indent=2)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[Skip] Error processing {entry.get('frontal')}: {str(e)}\")\n",
    "\n",
    "    logger.info(\"Completed. Saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
