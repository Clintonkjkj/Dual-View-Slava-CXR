{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ae37ae-8683-4a98-bd51-313b6a3d844d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 28647 train, 3183 test\n",
      "   'slava_llava_split_train.json'\n",
      "   'slava_llava_split_test.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "INPUT_JSON = \"radgraph_processed.json\"\n",
    "OUTPUT_JSON = \"slava_llava_split_\"\n",
    "TEST_SPLIT = 0.1# 10% test split\n",
    "\n",
    "# Use multi-image tokens (LLaVA-Phi expects these for dual images if trained that way)\n",
    "IMAGE_TOKEN = \"<image>\"\n",
    "\n",
    "# Prompt pools\n",
    "recognition_prompts = [\n",
    "    \"Enumerate all abnormal radiographic findings seen on the frontal and lateral chest X-rays, along with their precise anatomical locations.\",\n",
    "    \"List every visible pathology in the lungs, heart, pleura, and bones, as observed on both frontal and lateral views.\",\n",
    "    \"Describe only the radiographic abnormalities visible in these dual-view chest X-rays. Exclude normal structures.\",\n",
    "    \"Identify and localize any abnormal opacities, effusions, consolidations, or structural deviations present in the chest radiographs.\",\n",
    "    \"Specify all observed abnormalities in the dual chest views, including their type (e.g., mass, effusion, opacity) and anatomical location.\",\n",
    "    \"Report abnormal findings only from the provided frontal and lateral chest X-ray images. Do not describe normal appearances.\",\n",
    "    \"What pathological signs can be identified across both X-ray views? Be specific about laterality and anatomical regions.\",\n",
    "    \"Describe all clinically relevant radiographic findings, focusing on abnormalities in the lungs, mediastinum, and chest wall.\",\n",
    "    \"From the dual-view chest radiographs, list any deviations from normal radiographic anatomy or pathology that requires clinical attention.\",\n",
    "    \"Summarize all abnormal chest X-ray findings, organized by anatomical region (e.g., lungs, heart, pleura, bones).\"\n",
    "]\n",
    "\n",
    "\n",
    "reasoning_prompts = [\n",
    "    \"Based on the findings in the frontal and lateral chest X-rays, what is the most likely clinical diagnosis?\",\n",
    "    \"Interpret the radiographic abnormalities observed in both views and explain their clinical implications.\",\n",
    "    \"Given the dual-view chest radiographs, what is your diagnostic impression and reasoning behind it?\",\n",
    "    \"Using the observed abnormalities in these X-rays, infer the likely pathology and explain its clinical relevance.\",\n",
    "    \"What clinical condition best explains the abnormal findings visible in these frontal and lateral chest radiographs?\"\n",
    "]\n",
    "def impression_starts_with_normal_phrases(impression):\n",
    "    impression_text = impression.strip() if impression else ''\n",
    "    starts_with_patterns = [\n",
    "        r\"^no\\b\",\n",
    "        r\"^no evidence\\b\",\n",
    "        r\"^no acute\\b\",\n",
    "        r\"^normal\\b\"\n",
    "    ]\n",
    "\n",
    "    for pattern in starts_with_patterns:\n",
    "        if re.match(pattern, impression_text, re.IGNORECASE):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "def infer_view_hint(findings_text):\n",
    "    text_upper = findings_text.upper()\n",
    "    if \"PA\" in text_upper and \"AP\" in text_upper:\n",
    "        return \"PA and AP chest X-ray views are shown.\"\n",
    "    elif \"PA\" in text_upper:\n",
    "        return \"PA and lateral chest X-ray views are shown.\"\n",
    "    elif \"AP\" in text_upper:\n",
    "        return \"AP and lateral chest X-ray views are shown.\"\n",
    "    else:\n",
    "        return \"Frontal and lateral chest X-ray views are shown.\"\n",
    "\n",
    "# Load RadGraph-processed JSON\n",
    "with open(INPUT_JSON, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "output = []\n",
    "normalcount=0\n",
    "abnormalCount = 0\n",
    "for study_id, entry in data.items():\n",
    "    findings = entry.get(\"findings\", \"\").strip()\n",
    "    impression = entry.get(\"impression\", \"\").strip()\n",
    "    image_paths = entry.get(\"image_paths\", [])\n",
    "    frontal = entry.get(\"image_paths\", \"\")\n",
    "    lateral= entry.get(\"image_paths\", \"\")\n",
    "    if not findings or not impression:\n",
    "        continue\n",
    "    if len(image_paths) < 2:\n",
    "        continue  # Need both frontal and lateral views\n",
    "\n",
    "    # Dynamic view hint based on findings\n",
    "    view_hint = infer_view_hint(findings)\n",
    "\n",
    "    # Build prompts with image tokens and helpful context\n",
    "    recognition_prompt = f\"{IMAGE_TOKEN}{view_hint} {random.choice(recognition_prompts)}\"\n",
    "    reasoning_prompt = f\"{IMAGE_TOKEN}{view_hint} {random.choice(reasoning_prompts)}\"\n",
    "    item = {\n",
    "        \"frontal\": frontal[0],\n",
    "        \"lateral\": lateral[1],\n",
    "        \"recognition_input\": recognition_prompt,\n",
    "        \"reasoning_input\": reasoning_prompt,\n",
    "        \"findings\": findings,\n",
    "        \"impression\": impression\n",
    "    }\n",
    "    output.append(item)\n",
    "    \n",
    "    # if impression_starts_with_normal_phrases(impression) and normalcount<100:\n",
    "    #     output.append(item)\n",
    "    #     normalcount+=1\n",
    "    # elif not impression_starts_with_normal_phrases(impression):\n",
    "    #     abnormalCount+=1\n",
    "    #     output.append(item)  \n",
    "\n",
    "# Shuffle and split into train/test\n",
    "random.shuffle(output)\n",
    "split_index = int(len(output) * (1 - TEST_SPLIT))\n",
    "train_data = output[:split_index]\n",
    "test_data = output[split_index:]\n",
    "\n",
    "# Save to JSON files\n",
    "with open(OUTPUT_JSON + 'train.json', \"w\") as f:\n",
    "    json.dump(train_data, f, indent=2)\n",
    "\n",
    "with open(OUTPUT_JSON + 'test.json', \"w\") as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"âœ… Saved {len(train_data)} train, {len(test_data)} test\")\n",
    "print(f\"   '{OUTPUT_JSON}train.json'\")\n",
    "print(f\"   '{OUTPUT_JSON}test.json'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df584a2b-ff19-4e43-88b9-ae2441211005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Converted 28647 samples to slava_llava_recognition.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_json = \"slava_llava_split_train.json\"  # Original input\n",
    "output_json = \"slava_llava_recognition.json\"  # Output for LLaVA fine-tuning\n",
    "\n",
    "with open(input_json, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "converted = []\n",
    "\n",
    "for item in data:\n",
    "        try:\n",
    "            frontal = item[\"frontal\"]\n",
    "            lateral = item[\"lateral\"]\n",
    "            instruction = item[\"recognition_input\"]\n",
    "            response = item[\"findings\"]  # Yes, spelling is preserved as-is\n",
    "    \n",
    "            # Skip empty responses\n",
    "            if not response.strip():\n",
    "                continue\n",
    "    \n",
    "            sample = {\n",
    "                \"frontal\": frontal,\n",
    "                \"lateral\":lateral,\n",
    "                \"conversations\": [\n",
    "                    {\"from\": \"human\", \"value\": instruction},\n",
    "                    {\"from\": \"gpt\", \"value\": response}\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "            converted.append(sample)\n",
    "           \n",
    "    \n",
    "        except KeyError as e:\n",
    "            print(f\"[Skip] Missing key {e} in item\")\n",
    "\n",
    "        \n",
    "\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(converted, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Converted {len(converted)} samples to {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c79f4b-c146-4373-9b41-5473dcd3bd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts - Normal: 12755, Abnormal: 15892\n",
      "\n",
      "Final Dataset Composition:\n",
      "- abnormal_original: 15892 samples\n",
      "- abnormal_enhanced_2: 15892 samples\n",
      "- abnormal_enhanced_1: 15892 samples\n",
      "- abnormal_enhanced_3: 15892 samples\n",
      "- normal: 12755 samples\n",
      "\n",
      "âœ… Saved 76323 samples to slava_llava_reasoning.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "input_json = \"slava_llava_split_train.json\"\n",
    "output_json = \"slava_llava_reasoning.json\"\n",
    "\n",
    "# Load original data\n",
    "with open(input_json, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Categorize by normal/abnormal findings\n",
    "sample_buckets = defaultdict(list)\n",
    "for item in data:\n",
    "    impression = item.get(\"impression\", \"\").lower()\n",
    "    is_normal = \"no acute\" in impression or \"normal\" in impression or \"no evidence\" in impression\n",
    "    sample_buckets[\"normal\" if is_normal else \"abnormal\"].append(item)\n",
    "\n",
    "print(f\"Original counts - Normal: {len(sample_buckets['normal'])}, Abnormal: {len(sample_buckets['abnormal'])}\")\n",
    "\n",
    "# Oversampling parameters\n",
    "ABNORMAL_OVERSAMPLE_FACTOR = 3  # 3x more abnormal cases\n",
    "ENHANCED_PROMPT_TEMPLATES = [\n",
    "    \"GIVEN THESE FINDINGS: {findings}\\n{original_instruction}\",\n",
    "    \"THE RADIOLOGIST NOTED: {findings}\\nBASED ON THIS, {original_instruction}\",\n",
    "    \"CLINICAL CONTEXT: {findings}\\nPLEASE PROVIDE YOUR ANALYSIS: {original_instruction}\",\n",
    "    \"{original_instruction}\\nRELEVANT FINDINGS INCLUDE: {findings}\"\n",
    "]\n",
    "REASONING_PROMPTS = [\n",
    "    \"<image> ANALYZE THE DUAL-VIEW CHEST RADIOGRAPHS AND DESCRIBE THE MOST CLINICALLY SIGNIFICANT FINDINGS.\",\n",
    "    \"<image> IDENTIFY AND PRIORITIZE THE TOP 3 RADIOGRAPHIC ABNORMALITIES THAT REQUIRE CLINICAL ATTENTION.\",\n",
    "    \"<image> COMPARE THE CURRENT STUDY WITH PRIOR IMAGING (IF AVAILABLE). WHAT INTERVAL CHANGES ARE MOST CONCERNING?\",\n",
    "    \"<image> WHAT RADIOGRAPHIC SIGNS SUGGEST DECOMPENSATION IN THIS PATIENT'S CONDITION?\",\n",
    "    \"<image> WHICH FINDINGS WOULD YOU IMMEDIATELY REPORT TO THE TREATING PHYSICIAN AND WHY?\",\n",
    "    \"<image> ASSESS THE POSITIONING AND PLACEMENT OF ALL TUBES, LINES, AND DEVICES.\",\n",
    "    \"<image> DESCRIBE ANY FINDINGS THAT SUGGEST ACUTE VERSUS CHRONIC PATHOLOGICAL PROCESSES.\",\n",
    "    \"<image> WHAT DIFFERENTIAL DIAGNOSES WOULD YOU CONSIDER BASED ON THESE RADIOGRAPHIC FINDINGS?\",\n",
    "    \"<image> EVALUATE THE CARDIOPULMONARY STATUS AND COMMENT ON ANY DECOMPENSATION SIGNS.\",\n",
    "    \"<image> IDENTIFY ANY FINDINGS THAT MAY REQUIRE IMMEDIATE INTERVENTION VERSUS FOLLOW-UP MONITORING.\"\n",
    "]\n",
    "\n",
    "\n",
    "converted = []\n",
    "\n",
    "def build_enhanced_prompt(item):\n",
    "    \"\"\"Generate multiple prompt variations incorporating findings\"\"\"\n",
    "    original_instruction = random.choice(REASONING_PROMPTS)\n",
    "    findings = item.get(\"findings\", \"\").strip()\n",
    "    \n",
    "    if not findings:\n",
    "        return [original_instruction]\n",
    "    \n",
    "    return [\n",
    "        template.format(\n",
    "            findings=findings,\n",
    "            original_instruction=original_instruction\n",
    "        )\n",
    "        for template in ENHANCED_PROMPT_TEMPLATES\n",
    "    ]\n",
    "\n",
    "# Process normal samples (no oversampling)\n",
    "for item in sample_buckets[\"normal\"]:\n",
    "    try:\n",
    "        enhanced_prompts = build_enhanced_prompt(item)\n",
    "        converted.append({\n",
    "            \"frontal\": item[\"frontal\"],\n",
    "            \"lateral\": item[\"lateral\"],\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": enhanced_prompts[0]},\n",
    "                {\"from\": \"gpt\", \"value\": item[\"impression\"]}\n",
    "            ],\n",
    "            \"category\": \"normal\"\n",
    "        })\n",
    "    except KeyError as e:\n",
    "        print(f\"[Skip] Missing key {e} in normal sample\")\n",
    "\n",
    "# Process and oversample abnormal cases\n",
    "for item in sample_buckets[\"abnormal\"]:\n",
    "    try:\n",
    "        # Original sample\n",
    "        enhanced_prompts = build_enhanced_prompt(item)\n",
    "        converted.append({\n",
    "            \"frontal\": item[\"frontal\"],\n",
    "            \"lateral\": item[\"lateral\"],\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": enhanced_prompts[0] },\n",
    "                {\"from\": \"gpt\", \"value\": item[\"impression\"]}\n",
    "            ],\n",
    "            \"category\": \"abnormal_original\"\n",
    "        })\n",
    "        \n",
    "        # Create enhanced variations (oversampling)\n",
    "        for i, prompt in enumerate(enhanced_prompts[:ABNORMAL_OVERSAMPLE_FACTOR]):\n",
    "            converted.append({\n",
    "                \"frontal\": item[\"frontal\"],\n",
    "                \"lateral\": item[\"lateral\"],\n",
    "                \"conversations\": [\n",
    "                    {\"from\": \"human\", \"value\": prompt},\n",
    "                    {\"from\": \"gpt\", \"value\": item[\"impression\"]}\n",
    "                ],\n",
    "                \"category\": f\"abnormal_enhanced_{i+1}\"\n",
    "            })\n",
    "            \n",
    "    except KeyError as e:\n",
    "        print(f\"[Skip] Missing key {e} in abnormal sample\")\n",
    "\n",
    "# Shuffle the dataset to mix normal/abnormal samples\n",
    "random.shuffle(converted)\n",
    "\n",
    "# Save enhanced dataset\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(converted, f, indent=2)\n",
    "\n",
    "# Print statistics\n",
    "category_counts = defaultdict(int)\n",
    "for item in converted:\n",
    "    category_counts[item[\"category\"]] += 1\n",
    "\n",
    "print(\"\\nFinal Dataset Composition:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"- {category}: {count} samples\")\n",
    "print(f\"\\nâœ… Saved {len(converted)} samples to {output_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5aa46ef-2fb3-4e21-8f5f-bcf944bd8861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts - Normal: 12755, Abnormal: 15892\n",
      "\n",
      "Final Dataset Composition:\n",
      "- abnormal_original: 15892 samples\n",
      "- normal: 12755 samples\n",
      "- abnormal_enhanced_4: 15892 samples\n",
      "- abnormal_enhanced_2: 15892 samples\n",
      "- abnormal_enhanced_3: 15892 samples\n",
      "- abnormal_enhanced_1: 15892 samples\n",
      "\n",
      "âœ… Saved 92215 samples to slava_llava_report.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "input_json = \"slava_llava_split_train.json\"\n",
    "output_json = \"slava_llava_report.json\"\n",
    "\n",
    "# Load original data\n",
    "with open(input_json, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Categorize by normal/abnormal findings\n",
    "sample_buckets = defaultdict(list)\n",
    "for item in data:\n",
    "    impression = item.get(\"impression\", \"\").lower()\n",
    "    is_normal = \"no acute\" in impression or \"normal\" in impression or \"no evidence\" in impression\n",
    "    sample_buckets[\"normal\" if is_normal else \"abnormal\"].append(item)\n",
    "\n",
    "print(f\"Original counts - Normal: {len(sample_buckets['normal'])}, Abnormal: {len(sample_buckets['abnormal'])}\")\n",
    "\n",
    "# Oversampling parameters\n",
    "ABNORMAL_OVERSAMPLE_FACTOR = 4  # 3x more abnormal cases\n",
    "REPORT_INSTRUCTIONS = [\n",
    "    \"<image> Describe the findings in these frontal and lateral chest X-rays in a structured radiology report format.\",\n",
    "    \"<image> Generate a complete radiology report for these CXRs including findings and impression.\",\n",
    "    \"<image> Interpret these chest X-rays and provide a professional radiology report.\",\n",
    "    \"<image> What abnormalities are visible in these CXRs? Provide a structured report.\",\n",
    "    \"<image> Analyze these frontal and lateral chest X-rays and summarize the key findings.\",\n",
    "    \"<image> Prepare a radiology report for these images with findings and clinical impression.\",\n",
    "    \"<image> Evaluate these CXRs and document your observations in standard report format.\",\n",
    "    \"<image> Provide a detailed interpretation of these chest X-rays with findings and conclusion.\",\n",
    "    \"<image> Write a radiology report for these images following clinical documentation standards.\",\n",
    "    \"<image> Identify and describe any pathological findings in these CXRs in report format.\"\n",
    "]\n",
    "\n",
    "\n",
    "converted = []\n",
    "\n",
    "\n",
    "# Process normal samples (no oversampling)\n",
    "for item in sample_buckets[\"normal\"]:\n",
    "    try:\n",
    "        base_instruction = random.choice(REPORT_INSTRUCTIONS)\n",
    "        findings = item[\"findings\"]\n",
    "        impression = item[\"impression\"] \n",
    "        report_text = f\"FINDINGS: {findings}\"\n",
    "        report_text += f\"IMPRESSION: {impression}\"\n",
    "        converted.append({\n",
    "            \"frontal\": item[\"frontal\"],\n",
    "            \"lateral\": item[\"lateral\"],\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": base_instruction},\n",
    "                {\"from\": \"gpt\", \"value\":report_text}\n",
    "            ],\n",
    "            \"category\": \"normal\"\n",
    "        })\n",
    "    except KeyError as e:\n",
    "        print(f\"[Skip] Missing key {e} in normal sample\")\n",
    "\n",
    "# Process and oversample abnormal cases\n",
    "for item in sample_buckets[\"abnormal\"]:\n",
    "    try:\n",
    "        # Original sample\n",
    "        findings = item[\"findings\"]\n",
    "        impression = item[\"impression\"] \n",
    "        report_text = f\"FINDINGS: {findings}\"\n",
    "        report_text += f\"IMPRESSION: {impression}\"\n",
    "        converted.append({\n",
    "            \"frontal\": item[\"frontal\"],\n",
    "            \"lateral\": item[\"lateral\"],\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": random.choice(REPORT_INSTRUCTIONS) },\n",
    "                {\"from\": \"gpt\", \"value\":report_text}\n",
    "            ],\n",
    "            \"category\": \"abnormal_original\"\n",
    "        })\n",
    "        \n",
    "        # Create enhanced variations (oversampling)\n",
    "        for i in range(ABNORMAL_OVERSAMPLE_FACTOR):\n",
    "            converted.append({\n",
    "                \"frontal\": item[\"frontal\"],\n",
    "                \"lateral\": item[\"lateral\"],\n",
    "                \"conversations\": [\n",
    "                    {\"from\": \"human\", \"value\": random.choice(REPORT_INSTRUCTIONS)},\n",
    "                    {\"from\": \"gpt\", \"value\": report_text}\n",
    "                ],\n",
    "                \"category\": f\"abnormal_enhanced_{i+1}\"\n",
    "            })\n",
    "            \n",
    "    except KeyError as e:\n",
    "        print(f\"[Skip] Missing key {e} in abnormal sample\")\n",
    "\n",
    "# Shuffle the dataset to mix normal/abnormal samples\n",
    "random.shuffle(converted)\n",
    "\n",
    "# Save enhanced dataset\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(converted, f, indent=2)\n",
    "\n",
    "# Print statistics\n",
    "category_counts = defaultdict(int)\n",
    "for item in converted:\n",
    "    category_counts[item[\"category\"]] += 1\n",
    "\n",
    "print(\"\\nFinal Dataset Composition:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"- {category}: {count} samples\")\n",
    "print(f\"\\nâœ… Saved {len(converted)} samples to {output_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be45d684-f365-42e7-98a4-63522a1cfdd5",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/u24/miniforge/24.7.1-0/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:__main__:Loading model and tokenizer...\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.45s/it]\n",
      "INFO:__main__:Reading annotation file...\n",
      "INFO:__main__: Checking for existing output...\n",
      "INFO:__main__:Found 1615 previously processed entries. Skipping them...\n",
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3183/3183 [2:26:07<00:00,  2.75s/it]  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 271\u001b[0m\n\u001b[1;32m    268\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Completed. Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m entries to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_JSON\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[43mprocess_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 268\u001b[0m, in \u001b[0;36mprocess_validation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Skip] Error processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrontal\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# with open(OUTPUT_JSON, \"w\") as f:\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m#     json.dump(results, f, indent=2)\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Completed. Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mresults\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m entries to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_JSON\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import logging\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, CLIPImageProcessor\n",
    "from llava_phi.model import LlavaPhiForCausalLM\n",
    "from llava_phi.constants import DEFAULT_IMAGE_TOKEN\n",
    "from llava_phi.conversation import conv_templates\n",
    "from llava_phi.utils import disable_torch_init\n",
    "from transformers.generation.utils import GenerationMixin\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# ====================\n",
    "# ðŸ”§ CONFIGURATION\n",
    "# ====================\n",
    "MODEL_PATH = \"/media/volume/Slava/Dual-View-Slava-Final\"\n",
    "IMAGE_FOLDER = \"/media/volume/Slava/MIMIC_Dataset224\"\n",
    "INPUT_JSON = \"slava_llava_split_test.json\"\n",
    "OUTPUT_JSON = \"slava_llava_predict_1.json\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TORCH_DTYPE = torch.float16  # âœ… Faster\n",
    "MANUAL_SEED = 42\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "KEYWORDS = [\n",
    "    \"pneumonia\", \"atelectasis\", \"effusion\", \"cardiomegaly\", \"consolidation\",\n",
    "    \"edema\", \"scarring\", \"opacity\", \"vascular congestion\", \"catheter\", \"radiation\"\n",
    "]\n",
    "\n",
    "REPORT_INSTRUCTIONS = [\n",
    "    \"<image> You are a board-certified radiologist. Analyze the frontal and lateral chest X-rays. Identify and describe all radiographic findings, including location, size, density, and distribution. Provide a structured report with FINDINGS and IMPRESSION. Ensure clinical relevance and diagnostic precision.\",\n",
    "    \"<image> Evaluate the dual-view CXRs thoroughly. Identify all signs of disease, subtle or prominent. For each finding, describe anatomical position, severity, and possible differential diagnoses. Conclude with a well-reasoned and structured IMPRESSION.\",\n",
    "    \"<image> Generate a detailed radiology report using standard clinical format. Include all abnormal and incidental findings in the FINDINGS section. Summarize key diagnostic insights in the IMPRESSION. Avoid generic language; favor precise anatomical and pathological descriptors.\",\n",
    "    \"<image> Carefully examine the frontal and lateral chest X-rays. Write a complete radiology report structured into: FINDINGS (organized by organ system and zones) and IMPRESSION (summary with clinical prioritization). Include all deviations from normal, no matter how subtle.\",\n",
    "    \"<image> Interpret these dual-view CXRs. Document all radiologic abnormalities by describing their appearance, location (lung zones, mediastinum, pleura, bones), and clinical implications. Format the response as a formal radiology report with clear headings for FINDINGS and IMPRESSION.\",\n",
    "    \"<image> Examine the provided chest X-rays from both frontal and lateral views. Accurately identify and describe any abnormal radiologic signs, even subtle or borderline cases. Structure the report with FINDINGS and a diagnostic IMPRESSION.\",\n",
    "    \"<image> Perform a radiological assessment of these dual-view CXRs. List all notable observations and pathological signs, with attention to anatomical detail and clinical context. Format the response as a full report: FINDINGS followed by IMPRESSION.\",\n",
    "    \"<image> Review the chest radiographs and generate a report that includes all visible abnormalities. Pay attention to symmetry, lung markings, cardiac silhouette, and bony structures. Use the standard format with FINDINGS and IMPRESSION.\",\n",
    "    \"<image> You are an expert thoracic radiologist. Describe all pathological and incidental findings in these frontal and lateral CXRs. Be thorough and concise. Conclude with a structured IMPRESSION summarizing the clinical picture.\",\n",
    "    \"<image> Analyze these chest X-rays using your clinical expertise. Report every abnormality using specific radiologic terminology. Clearly differentiate FINDINGS and IMPRESSION. Include zone-wise, side-wise, and severity-based descriptions.\"\n",
    "]\n",
    "\n",
    "\n",
    "# ====================\n",
    "# ðŸ“‹ LOGGING SETUP\n",
    "# ====================\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ====================\n",
    "# ðŸ§  MODEL WRAPPER\n",
    "# ====================\n",
    "class LlavaWithGenerate(LlavaPhiForCausalLM, GenerationMixin):\n",
    "    def generate(self, *args, **kwargs):\n",
    "        self.image_features = None\n",
    "        self.images = kwargs.pop(\"images\", None)\n",
    "        return super().generate(*args, **kwargs)\n",
    "\n",
    "# ====================\n",
    "# LOAD MODEL + TOKENIZER\n",
    "# ====================\n",
    "def load_model_and_tokenizer():\n",
    "    disable_torch_init()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    if \"<image>\" not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = LlavaWithGenerate.from_pretrained(MODEL_PATH, torch_dtype=TORCH_DTYPE, low_cpu_mem_usage=True)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    model.model.bypass_vision_tower = False\n",
    "    image_token_id = tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "    assert image_token_id < model.get_input_embeddings().num_embeddings\n",
    "\n",
    "    if torch.__version__ >= \"2\" and torch.cuda.is_available():\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"torch.compile() failed: {e}\")\n",
    "\n",
    "    return model, tokenizer, image_token_id\n",
    "\n",
    "# ====================\n",
    "# CONVERSATION FORMATTER\n",
    "# ====================\n",
    "def prepare_conversation(prompt):\n",
    "    template_name = next((name for name in [\"llava_phi\", \"phi\", \"v0\", \"med\", \"vicuna_v1\"] if name in conv_templates), None)\n",
    "    conv = conv_templates[template_name].copy()\n",
    "    conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "# ====================\n",
    "# IMAGE PROCESSING\n",
    "# ====================\n",
    "IMAGE_PROCESSOR = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "\n",
    "def expand2square(pil_img, background_color=(0, 0, 0)):\n",
    "    w, h = pil_img.size\n",
    "    if w == h:\n",
    "        return pil_img\n",
    "    elif w > h:\n",
    "        result = Image.new(pil_img.mode, (w, w), background_color)\n",
    "        result.paste(pil_img, (0, (w - h) // 2))\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (h, h), background_color)\n",
    "        result.paste(pil_img, ((h - w) // 2, 0))\n",
    "    return result\n",
    "\n",
    "def prepare_images(frontal_path, lateral_path):\n",
    "    image_paths = [frontal_path, lateral_path]\n",
    "    images = []\n",
    "    bg_color = tuple(int(x * 255) for x in IMAGE_PROCESSOR.image_mean)\n",
    "    for img_path in image_paths:\n",
    "        with Image.open(os.path.join(IMAGE_FOLDER, img_path)) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "            img = expand2square(img, bg_color)\n",
    "            image_tensor = IMAGE_PROCESSOR(img, return_tensors=\"pt\")['pixel_values'][0]\n",
    "            images.append(image_tensor)\n",
    "    return torch.stack(images).contiguous()\n",
    "\n",
    "# ====================\n",
    "# TOKENIZER IMAGE HANDLING\n",
    "# ====================\n",
    "def tokenizer_image_token(prompt, tokenizer, image_token_index, return_tensors=None):\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors == 'pt':\n",
    "        return torch.tensor(input_ids, dtype=torch.long)\n",
    "    return input_ids\n",
    "\n",
    "# ====================\n",
    "# HEURISTIC CHECKS\n",
    "# ====================\n",
    "def is_low_confidence(text: str, min_word_count=40):\n",
    "    return len(text.split()) < min_word_count or \"impression:\" not in text.lower()\n",
    "\n",
    "def must_mention_keywords(reference: str, prediction: str, keywords=KEYWORDS):\n",
    "    ref_keywords = [kw for kw in keywords if kw in reference.lower()]\n",
    "    if not ref_keywords:\n",
    "        return False\n",
    "    missing = [kw for kw in ref_keywords if kw not in prediction.lower()]\n",
    "    return len(missing) > 0\n",
    "\n",
    "# ====================\n",
    "# GENERATION LOGIC\n",
    "# ====================\n",
    "def generate_response(model, tokenizer, images, prompt, reference, image_token_index, max_retries=1):\n",
    "    regen_reason = None\n",
    "    for attempt in range(max_retries + 1):\n",
    "        full_prompt = prepare_conversation(prompt)\n",
    "        input_ids = tokenizer_image_token(full_prompt, tokenizer, image_token_index, return_tensors=\"pt\").unsqueeze(0).to(DEVICE)\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).long().to(DEVICE)\n",
    "        images = images.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(device_type='cuda', dtype=TORCH_DTYPE):\n",
    "            try:\n",
    "                output_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    images=images,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.95,\n",
    "                    max_new_tokens=1024,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            except RuntimeError as e:\n",
    "                if \"indexSelectLargeIndex\" in str(e):\n",
    "                    return \"[ERROR: CUDA index bug]\", True, \"cuda_index_error\"\n",
    "                raise e\n",
    "\n",
    "        gen_tokens = output_ids[0, input_ids.shape[1]:]\n",
    "        decoded = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        if is_low_confidence(decoded):\n",
    "            regen_reason = \"low_confidence\"\n",
    "            continue\n",
    "        if must_mention_keywords(reference, decoded):\n",
    "            regen_reason = \"missing_keywords\"\n",
    "            continue\n",
    "\n",
    "        return decoded, (attempt > 0), regen_reason\n",
    "\n",
    "    return decoded, True, regen_reason or \"max_attempts\"\n",
    "\n",
    "# ====================\n",
    "# MAIN LOOP\n",
    "# ====================\n",
    "def process_validation():\n",
    "    logger.info(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer, image_token_index = load_model_and_tokenizer()\n",
    "    torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "    logger.info(\"Reading annotation file...\")\n",
    "    with open(INPUT_JSON, \"r\") as f:\n",
    "        val_data = json.load(f)\n",
    "    # val_data = val_data[:1000]  \n",
    "    logger.info(\" Checking for existing output...\")\n",
    "    existing_predictions = {}\n",
    "    if os.path.exists(OUTPUT_JSON):\n",
    "        with open(OUTPUT_JSON, \"r\") as f:\n",
    "            for item in json.load(f):\n",
    "                key = f\"{item['frontal']}|{item['lateral']}\"\n",
    "                existing_predictions[key] = item\n",
    "\n",
    "    processed_keys = set(existing_predictions.keys())\n",
    "    logger.info(f\"Found {len(processed_keys)} previously processed entries. Skipping them...\")\n",
    "    for entry in tqdm(val_data, desc=\"Processing\"):\n",
    "        try:\n",
    "            findings = entry.get(\"findings\")\n",
    "            impression = entry.get(\"impression\")\n",
    "            reference_text = f\"{findings}. IMPRESSION: {impression}\" if findings and impression else \"\"\n",
    "            frontal = entry.get(\"frontal\")\n",
    "            lateral = entry.get(\"lateral\")\n",
    "            key = f\"{frontal}|{lateral}\"\n",
    "\n",
    "            if key in processed_keys:\n",
    "                continue\n",
    "            prompt = random.choice(REPORT_INSTRUCTIONS)\n",
    "\n",
    "            if not frontal or not lateral or not reference_text:\n",
    "                continue\n",
    "\n",
    "            images = prepare_images(frontal, lateral)\n",
    "            prediction, was_regenerated, regen_reason = generate_response(\n",
    "                model, tokenizer, images, prompt, reference_text, image_token_index\n",
    "            )\n",
    "\n",
    "            result={\n",
    "                \"frontal\": frontal,\n",
    "                \"lateral\": lateral,\n",
    "                \"prompt\": prompt,\n",
    "                \"reference\": reference_text,\n",
    "                \"prediction\": prediction,\n",
    "                # \"regenerated\": was_regenerated,\n",
    "                # \"regeneration_reason\": regen_reason if was_regenerated else None\n",
    "            }\n",
    "            existing_predictions[key] = result\n",
    "            with open(OUTPUT_JSON, \"w\") as f:\n",
    "                    json.dump(list(existing_predictions.values()), f, indent=2)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[Skip] Error processing {entry.get('frontal')}: {str(e)}\")\n",
    "\n",
    "    # with open(OUTPUT_JSON, \"w\") as f:\n",
    "    #     json.dump(results, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Completed. Saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_validation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ae488-19bb-4953-a044-8dfdeae62dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import logging\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, CLIPImageProcessor\n",
    "from llava_phi.model import LlavaPhiForCausalLM\n",
    "from llava_phi.constants import DEFAULT_IMAGE_TOKEN\n",
    "from llava_phi.conversation import conv_templates\n",
    "from llava_phi.utils import disable_torch_init\n",
    "from transformers.generation.utils import GenerationMixin\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# ====================\n",
    "# ðŸ”§ CONFIGURATION\n",
    "# ====================\n",
    "MODEL_PATH = \"/media/volume/Slava/Dual-View-Slava-Final\"\n",
    "IMAGE_FOLDER = \"/media/volume/Slava/IU_Xray\"\n",
    "INPUT_JSON = \"slava_llava_split_test.json\"\n",
    "OUTPUT_JSON = \"slava_llava_predict_1.json\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TORCH_DTYPE = torch.float16  # âœ… Faster\n",
    "MANUAL_SEED = 42\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "KEYWORDS = [\n",
    "    \"pneumonia\", \"atelectasis\", \"effusion\", \"cardiomegaly\", \"consolidation\",\n",
    "    \"edema\", \"scarring\", \"opacity\", \"vascular congestion\", \"catheter\", \"radiation\"\n",
    "]\n",
    "\n",
    "REPORT_INSTRUCTIONS = [\n",
    "    \"<image> You are a board-certified radiologist. Analyze the frontal and lateral chest X-rays. Identify and describe all radiographic findings, including location, size, density, and distribution. Provide a structured report with FINDINGS and IMPRESSION. Ensure clinical relevance and diagnostic precision.\",\n",
    "    \"<image> Evaluate the dual-view CXRs thoroughly. Identify all signs of disease, subtle or prominent. For each finding, describe anatomical position, severity, and possible differential diagnoses. Conclude with a well-reasoned and structured IMPRESSION.\",\n",
    "    \"<image> Generate a detailed radiology report using standard clinical format. Include all abnormal and incidental findings in the FINDINGS section. Summarize key diagnostic insights in the IMPRESSION. Avoid generic language; favor precise anatomical and pathological descriptors.\",\n",
    "    \"<image> Carefully examine the frontal and lateral chest X-rays. Write a complete radiology report structured into: FINDINGS (organized by organ system and zones) and IMPRESSION (summary with clinical prioritization). Include all deviations from normal, no matter how subtle.\",\n",
    "    \"<image> Interpret these dual-view CXRs. Document all radiologic abnormalities by describing their appearance, location (lung zones, mediastinum, pleura, bones), and clinical implications. Format the response as a formal radiology report with clear headings for FINDINGS and IMPRESSION.\",\n",
    "    \"<image> Examine the provided chest X-rays from both frontal and lateral views. Accurately identify and describe any abnormal radiologic signs, even subtle or borderline cases. Structure the report with FINDINGS and a diagnostic IMPRESSION.\",\n",
    "    \"<image> Perform a radiological assessment of these dual-view CXRs. List all notable observations and pathological signs, with attention to anatomical detail and clinical context. Format the response as a full report: FINDINGS followed by IMPRESSION.\",\n",
    "    \"<image> Review the chest radiographs and generate a report that includes all visible abnormalities. Pay attention to symmetry, lung markings, cardiac silhouette, and bony structures. Use the standard format with FINDINGS and IMPRESSION.\",\n",
    "    \"<image> You are an expert thoracic radiologist. Describe all pathological and incidental findings in these frontal and lateral CXRs. Be thorough and concise. Conclude with a structured IMPRESSION summarizing the clinical picture.\",\n",
    "    \"<image> Analyze these chest X-rays using your clinical expertise. Report every abnormality using specific radiologic terminology. Clearly differentiate FINDINGS and IMPRESSION. Include zone-wise, side-wise, and severity-based descriptions.\"\n",
    "]\n",
    "\n",
    "\n",
    "# ====================\n",
    "# ðŸ“‹ LOGGING SETUP\n",
    "# ====================\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ====================\n",
    "# ðŸ§  MODEL WRAPPER\n",
    "# ====================\n",
    "class LlavaWithGenerate(LlavaPhiForCausalLM, GenerationMixin):\n",
    "    def generate(self, *args, **kwargs):\n",
    "        self.image_features = None\n",
    "        self.images = kwargs.pop(\"images\", None)\n",
    "        return super().generate(*args, **kwargs)\n",
    "\n",
    "# ====================\n",
    "# LOAD MODEL + TOKENIZER\n",
    "# ====================\n",
    "def load_model_and_tokenizer():\n",
    "    disable_torch_init()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    if \"<image>\" not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = LlavaWithGenerate.from_pretrained(MODEL_PATH, torch_dtype=TORCH_DTYPE, low_cpu_mem_usage=True)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    model.model.bypass_vision_tower = False\n",
    "    image_token_id = tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "    assert image_token_id < model.get_input_embeddings().num_embeddings\n",
    "\n",
    "    if torch.__version__ >= \"2\" and torch.cuda.is_available():\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"torch.compile() failed: {e}\")\n",
    "\n",
    "    return model, tokenizer, image_token_id\n",
    "\n",
    "# ====================\n",
    "# CONVERSATION FORMATTER\n",
    "# ====================\n",
    "def prepare_conversation(prompt):\n",
    "    template_name = next((name for name in [\"llava_phi\", \"phi\", \"v0\", \"med\", \"vicuna_v1\"] if name in conv_templates), None)\n",
    "    conv = conv_templates[template_name].copy()\n",
    "    conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "# ====================\n",
    "# IMAGE PROCESSING\n",
    "# ====================\n",
    "IMAGE_PROCESSOR = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "\n",
    "def expand2square(pil_img, background_color=(0, 0, 0)):\n",
    "    w, h = pil_img.size\n",
    "    if w == h:\n",
    "        return pil_img\n",
    "    elif w > h:\n",
    "        result = Image.new(pil_img.mode, (w, w), background_color)\n",
    "        result.paste(pil_img, (0, (w - h) // 2))\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (h, h), background_color)\n",
    "        result.paste(pil_img, ((h - w) // 2, 0))\n",
    "    return result\n",
    "\n",
    "def prepare_images(frontal_path, lateral_path):\n",
    "    image_paths = [frontal_path, lateral_path]\n",
    "    images = []\n",
    "    bg_color = tuple(int(x * 255) for x in IMAGE_PROCESSOR.image_mean)\n",
    "    for img_path in image_paths:\n",
    "        with Image.open(os.path.join(IMAGE_FOLDER, img_path)) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "            img = expand2square(img, bg_color)\n",
    "            image_tensor = IMAGE_PROCESSOR(img, return_tensors=\"pt\")['pixel_values'][0]\n",
    "            images.append(image_tensor)\n",
    "    return torch.stack(images).contiguous()\n",
    "\n",
    "# ====================\n",
    "# TOKENIZER IMAGE HANDLING\n",
    "# ====================\n",
    "def tokenizer_image_token(prompt, tokenizer, image_token_index, return_tensors=None):\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors == 'pt':\n",
    "        return torch.tensor(input_ids, dtype=torch.long)\n",
    "    return input_ids\n",
    "\n",
    "# ====================\n",
    "# HEURISTIC CHECKS\n",
    "# ====================\n",
    "def is_low_confidence(text: str, min_word_count=40):\n",
    "    return len(text.split()) < min_word_count or \"impression:\" not in text.lower()\n",
    "\n",
    "def must_mention_keywords(reference: str, prediction: str, keywords=KEYWORDS):\n",
    "    ref_keywords = [kw for kw in keywords if kw in reference.lower()]\n",
    "    if not ref_keywords:\n",
    "        return False\n",
    "    missing = [kw for kw in ref_keywords if kw not in prediction.lower()]\n",
    "    return len(missing) > 0\n",
    "\n",
    "# ====================\n",
    "# GENERATION LOGIC\n",
    "# ====================\n",
    "def generate_response(model, tokenizer, images, prompt, reference, image_token_index, max_retries=1):\n",
    "    regen_reason = None\n",
    "    for attempt in range(max_retries + 1):\n",
    "        full_prompt = prepare_conversation(prompt)\n",
    "        input_ids = tokenizer_image_token(full_prompt, tokenizer, image_token_index, return_tensors=\"pt\").unsqueeze(0).to(DEVICE)\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).long().to(DEVICE)\n",
    "        images = images.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(device_type='cuda', dtype=TORCH_DTYPE):\n",
    "            try:\n",
    "                output_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    images=images,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.95,\n",
    "                    max_new_tokens=1024,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            except RuntimeError as e:\n",
    "                if \"indexSelectLargeIndex\" in str(e):\n",
    "                    return \"[ERROR: CUDA index bug]\", True, \"cuda_index_error\"\n",
    "                raise e\n",
    "\n",
    "        gen_tokens = output_ids[0, input_ids.shape[1]:]\n",
    "        decoded = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        if is_low_confidence(decoded):\n",
    "            regen_reason = \"low_confidence\"\n",
    "            continue\n",
    "        if must_mention_keywords(reference, decoded):\n",
    "            regen_reason = \"missing_keywords\"\n",
    "            continue\n",
    "\n",
    "        return decoded, (attempt > 0), regen_reason\n",
    "\n",
    "    return decoded, True, regen_reason or \"max_attempts\"\n",
    "\n",
    "# ====================\n",
    "# MAIN LOOP\n",
    "# ====================\n",
    "def process_validation():\n",
    "    logger.info(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer, image_token_index = load_model_and_tokenizer()\n",
    "    torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "    logger.info(\"Reading annotation file...\")\n",
    "    with open(INPUT_JSON, \"r\") as f:\n",
    "        val_data = json.load(f)\n",
    "    # val_data = val_data[:1000]  \n",
    "    logger.info(\" Checking for existing output...\")\n",
    "    existing_predictions = {}\n",
    "    if os.path.exists(OUTPUT_JSON):\n",
    "        with open(OUTPUT_JSON, \"r\") as f:\n",
    "            for item in json.load(f):\n",
    "                key = f\"{item['frontal']}|{item['lateral']}\"\n",
    "                existing_predictions[key] = item\n",
    "\n",
    "    processed_keys = set(existing_predictions.keys())\n",
    "    logger.info(f\"Found {len(processed_keys)} previously processed entries. Skipping them...\")\n",
    "    for entry in tqdm(val_data, desc=\"Processing\"):\n",
    "        try:\n",
    "            findings = entry.get(\"findings\")\n",
    "            impression = entry.get(\"impression\")\n",
    "            reference_text = f\"{findings}. IMPRESSION: {impression}\" if findings and impression else \"\"\n",
    "            frontal = entry.get(\"frontal\")\n",
    "            lateral = entry.get(\"lateral\")\n",
    "            key = f\"{frontal}|{lateral}\"\n",
    "\n",
    "            if key in processed_keys:\n",
    "                continue\n",
    "            prompt = random.choice(REPORT_INSTRUCTIONS)\n",
    "\n",
    "            if not frontal or not lateral or not reference_text:\n",
    "                continue\n",
    "\n",
    "            images = prepare_images(frontal, lateral)\n",
    "            prediction, was_regenerated, regen_reason = generate_response(\n",
    "                model, tokenizer, images, prompt, reference_text, image_token_index\n",
    "            )\n",
    "\n",
    "            result={\n",
    "                \"frontal\": frontal,\n",
    "                \"lateral\": lateral,\n",
    "                \"prompt\": prompt,\n",
    "                \"reference\": reference_text,\n",
    "                \"prediction\": prediction,\n",
    "                # \"regenerated\": was_regenerated,\n",
    "                # \"regeneration_reason\": regen_reason if was_regenerated else None\n",
    "            }\n",
    "            existing_predictions[key] = result\n",
    "            with open(OUTPUT_JSON, \"w\") as f:\n",
    "                    json.dump(list(existing_predictions.values()), f, indent=2)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[Skip] Error processing {entry.get('frontal')}: {str(e)}\")\n",
    "\n",
    "    # with open(OUTPUT_JSON, \"w\") as f:\n",
    "    #     json.dump(results, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Completed. Saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_validation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8c816-d65a-4fcc-b59b-9ebc7ed79ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import logging\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, CLIPImageProcessor\n",
    "from llava_phi.model import LlavaPhiForCausalLM\n",
    "from llava_phi.constants import DEFAULT_IMAGE_TOKEN\n",
    "from llava_phi.conversation import conv_templates\n",
    "from llava_phi.utils import disable_torch_init\n",
    "from transformers.generation.utils import GenerationMixin\n",
    "\n",
    "# ====================\n",
    "# ðŸ”§ CONFIGURATION\n",
    "# ====================\n",
    "MODEL_PATH = \"/media/volume/Slava/Dual-View-Slava/Reporting\"\n",
    "IMAGE_FOLDER = \"/media/volume/Slava/MIMIC_Dataset224\"\n",
    "INPUT_JSON = \"slava_llava_split_test.json\"\n",
    "OUTPUT_JSON = \"slava_llava_predict.json\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TORCH_DTYPE = torch.float32\n",
    "MANUAL_SEED = 42\n",
    "\n",
    "REPORT_INSTRUCTIONS = [\n",
    "    \"<image> Carefully analyze the frontal and lateral chest X-rays. Describe all observed abnormalities in detail, including anatomical location and severity. Conclude with a structured impression.\",\n",
    "    \"<image> Generate a detailed radiology report including both FINDINGS and IMPRESSION. Focus especially on any abnormal features or clinically relevant signs.\",\n",
    "    \"<image> Write a complete and structured report for these dual-view CXRs. Prioritize abnormal observations and explain their possible clinical implications.\",\n",
    "    \"<image> Interpret these chest X-rays. Clearly describe all abnormal findings and avoid assumptions of normality unless explicitly visible. Provide a structured radiology report.\",\n",
    "    \"<image> Create a full radiology report with FINDINGS and IMPRESSION. Do not omit any subtle or mild abnormalities seen in the images.\",\n",
    "    \"<image> Analyze the given chest X-rays (frontal and lateral). List all pathological findings, even minor ones, and summarize them in the impression.\",\n",
    "    \"<image> Provide a radiology report describing all radiographic abnormalities in detail. Include location, extent, and possible causes. Format it as FINDINGS and IMPRESSION.\",\n",
    "    \"<image> You are a radiologist. Examine these CXRs and report every deviation from normal. Follow a standard structured reporting format.\",\n",
    "    \"<image> Describe all visible pathologies and incidental findings in this CXR study. Use clinical terminology and conclude with a professional impression.\",\n",
    "    \"<image> These are chest X-rays showing potential abnormalities. Write a radiology report that prioritizes accuracy and completeness, mentioning all visible signs of disease.\"\n",
    "]\n",
    "\n",
    "KEYWORDS = [\n",
    "    \"pneumonia\", \"effusion\", \"pneumothorax\", \"consolidation\", \"atelectasis\", \"edema\", \"opacity\", \"pleural\",\n",
    "    \"pulmonary\", \"lung\", \"lungs\", \"cardiomegaly\", \"enlargement\", \"focal\", \"infiltrate\", \"interstitial\",\n",
    "    \"scarring\", \"fibrosis\", \"calcification\", \"mass\", \"nodule\", \"hyperinflation\", \"collapse\", \"thickening\",\n",
    "    \"blunting\", \"catheter\", \"tubes\", \"lines\", \"hernia\", \"fracture\", \"kyphosis\", \"lucency\", \"emphysema\",\n",
    "    \"silhouette\", \"hilar\", \"cardiomediastinal\", \"mediastinal\", \"vascular congestion\"\n",
    "]\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LlavaWithGenerate(LlavaPhiForCausalLM, GenerationMixin):\n",
    "    def generate(self, *args, **kwargs):\n",
    "        self.image_features = None\n",
    "        self.images = kwargs.pop(\"images\", None)\n",
    "        return super().generate(*args, **kwargs)\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    disable_torch_init()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = LlavaWithGenerate.from_pretrained(MODEL_PATH, torch_dtype=TORCH_DTYPE, low_cpu_mem_usage=True)\n",
    "    model = model.to(DEVICE)\n",
    "    model.model.bypass_vision_tower = False\n",
    "    return model, tokenizer\n",
    "\n",
    "IMAGE_PROCESSOR = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "\n",
    "def expand2square(pil_img, background_color=(0, 0, 0)):\n",
    "    w, h = pil_img.size\n",
    "    if w == h:\n",
    "        return pil_img\n",
    "    result = Image.new(pil_img.mode, (max(w, h), max(w, h)), background_color)\n",
    "    result.paste(pil_img, ((max(w, h) - w) // 2, (max(w, h) - h) // 2))\n",
    "    return result\n",
    "\n",
    "def prepare_images(frontal_path, lateral_path):\n",
    "    image_paths = [frontal_path, lateral_path]\n",
    "    images = []\n",
    "    for img_path in image_paths:\n",
    "        img = Image.open(os.path.join(IMAGE_FOLDER, img_path)).convert(\"RGB\")\n",
    "        img = expand2square(img, tuple(int(x * 255) for x in IMAGE_PROCESSOR.image_mean))\n",
    "        image_tensor = IMAGE_PROCESSOR(img, return_tensors=\"pt\")['pixel_values'][0]\n",
    "        images.append(image_tensor)\n",
    "    return torch.stack(images)\n",
    "\n",
    "def prepare_conversation(prompt):\n",
    "    template_name = next((name for name in [\"llava_phi\", \"phi\", \"v0\", \"med\", \"vicuna_v1\"] if name in conv_templates), None)\n",
    "    conv = conv_templates[template_name].copy()\n",
    "    conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "def is_low_confidence(text: str, min_word_count=40):\n",
    "    return len(text.split()) < min_word_count or \"impression:\" not in text.lower()\n",
    "\n",
    "def must_mention_keywords(reference: str, prediction: str, keywords=KEYWORDS):\n",
    "    ref_keywords = [kw for kw in keywords if kw in reference.lower()]\n",
    "    return any(kw not in prediction.lower() for kw in ref_keywords)\n",
    "\n",
    "def is_semantically_mismatched(reference, prediction, threshold=0.85):\n",
    "    if not prediction.strip() or not reference.strip():\n",
    "        return True, 0.0\n",
    "    try:\n",
    "        P, R, F1 = bert_score([prediction], [reference], lang=\"en\", rescale_with_baseline=False)\n",
    "        return F1[0].item() < threshold, F1[0].item()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"BERTScore failed: {e}\")\n",
    "        return True, 0.0\n",
    "\n",
    "def generate_response(model, tokenizer, images, prompt, reference, max_retries=2):\n",
    "    regen_reason = None\n",
    "    best_f1 = 0.0\n",
    "    generation_settings = [\n",
    "        {\"temperature\": 0.4, \"top_p\": 0.9, \"max_new_tokens\": 768},\n",
    "        {\"temperature\": 0.2, \"top_p\": 0.85, \"max_new_tokens\": 896},\n",
    "        {\"temperature\": 0.1, \"top_p\": 0.80, \"max_new_tokens\": 1024}\n",
    "    ]\n",
    "\n",
    "    for attempt in range(max_retries + 1):\n",
    "        settings = generation_settings[min(attempt, len(generation_settings) - 1)]\n",
    "        full_prompt = prepare_conversation(prompt)\n",
    "        tokenized = tokenizer(\n",
    "            full_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=tokenizer.model_max_length - 512\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = tokenized[\"attention_mask\"].to(DEVICE)\n",
    "        images = images.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=TORCH_DTYPE == torch.bfloat16):\n",
    "            output_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                do_sample=True,\n",
    "                temperature=settings[\"temperature\"],\n",
    "                top_p=settings[\"top_p\"],\n",
    "                max_new_tokens=settings[\"max_new_tokens\"],\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        gen_tokens = output_ids[0, input_ids.shape[1]:]\n",
    "        decoded = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        if is_low_confidence(decoded):\n",
    "            regen_reason = \"low_confidence\"\n",
    "            continue\n",
    "\n",
    "        if must_mention_keywords(reference, decoded):\n",
    "            regen_reason = \"missing_keywords\"\n",
    "            continue\n",
    "\n",
    "        mismatch, f1_score = is_semantically_mismatched(reference, decoded)\n",
    "        best_f1 = f1_score\n",
    "        if mismatch:\n",
    "            regen_reason = f\"low_bertscore ({f1_score:.3f})\"\n",
    "            continue\n",
    "\n",
    "        return decoded, (attempt > 0), None, round(f1_score, 4)\n",
    "\n",
    "    return decoded, True, regen_reason or \"max_attempts\", round(best_f1, 4)\n",
    "\n",
    "def process_validation():\n",
    "    logger.info(\"ðŸ“¦ Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "    logger.info(\"ðŸ“– Reading annotation file...\")\n",
    "    with open(INPUT_JSON, \"r\") as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    logger.info(\"ðŸ—‚ï¸  Checking for existing output...\")\n",
    "    existing_predictions = {}\n",
    "    if os.path.exists(OUTPUT_JSON):\n",
    "        with open(OUTPUT_JSON, \"r\") as f:\n",
    "            for item in json.load(f):\n",
    "                key = f\"{item['frontal']}|{item['lateral']}\"\n",
    "                existing_predictions[key] = item\n",
    "\n",
    "    processed_keys = set(existing_predictions.keys())\n",
    "    logger.info(f\"ðŸ” Found {len(processed_keys)} previously processed entries. Skipping them...\")\n",
    "\n",
    "    logger.info(\"ðŸ§š Running inference...\")\n",
    "    for entry in tqdm(val_data, desc=\"Processing\"):\n",
    "        try:\n",
    "            frontal = entry.get(\"frontal\")\n",
    "            lateral = entry.get(\"lateral\")\n",
    "            key = f\"{frontal}|{lateral}\"\n",
    "\n",
    "            if key in processed_keys:\n",
    "                continue\n",
    "\n",
    "            findings = entry.get(\"findings\")\n",
    "            impression = entry.get(\"impression\")\n",
    "            reference_text = f\"{findings}. IMPRESSION: {impression}\" if findings and impression else \"\"\n",
    "            prompt = random.choice(REPORT_INSTRUCTIONS)\n",
    "\n",
    "            if not frontal or not lateral or not reference_text:\n",
    "                continue\n",
    "\n",
    "            images = prepare_images(frontal, lateral)\n",
    "            prediction, was_regenerated, regen_reason, bert_f1 = generate_response(\n",
    "                model, tokenizer, images, prompt, reference_text\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"frontal\": frontal,\n",
    "                \"lateral\": lateral,\n",
    "                \"prompt\": prompt,\n",
    "                \"reference\": reference_text,\n",
    "                \"prediction\": prediction,\n",
    "                \"regenerated\": was_regenerated,\n",
    "                \"regeneration_reason\": regen_reason,\n",
    "                \"bertscore_f1\": bert_f1\n",
    "            }\n",
    "\n",
    "            existing_predictions[key] = result\n",
    "\n",
    "            with open(OUTPUT_JSON, \"w\") as f:\n",
    "                json.dump(list(existing_predictions.values()), f, indent=2)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[Skip] Error processing {entry.get('frontal')}: {str(e)}\")\n",
    "\n",
    "    logger.info(f\"âœ… Finished. Total entries saved: {len(existing_predictions)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_validation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701243b6-3414-4b7d-a717-0bf0cf1932d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
